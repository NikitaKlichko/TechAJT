{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, matthews_corrcoef\n",
    "from sklearn.utils import class_weight\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_ajt_df.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"test_ajt_df.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./ajt_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, df.loc[train_df.index]['type_mistake'], ], axis=1)\n",
    "test_df = pd.concat([test_df, df.loc[test_df.index]['type_mistake'], ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapper = {\"нет ошибки\": -1, \"речевая\": 0, \"стилистическая\": 1, \"пунктуационная\": 2, \"грамматическая\": 3, \"лексическая\": 4, \"логическая\": 5}\n",
    "reverse_label_mapper = {-1: \"нет ошибки\", 0: \"речевая\", 1: \"стилистическая\", 2: \"пунктуационная\", 3: \"грамматическая\", 4: \"лексическая\", 5: \"логическая\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['type_mistake'] = train_df['type_mistake'].map(label_mapper)\n",
    "test_df['type_mistake'] = test_df['type_mistake'].map(label_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if ru-en-RosBerta\n",
    "# prefix = \"classification: \"\n",
    "# train_df['text'] = prefix + train_df['text'] \n",
    "# test_df['text'] = prefix + test_df['text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, binary_labels, error_labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.binary_labels = binary_labels\n",
    "        self.error_labels = error_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'binary_label': torch.tensor(self.binary_labels[idx], dtype=torch.float),\n",
    "            'error_label': torch.tensor(self.error_labels[idx], dtype=torch.float)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pooling(outputs, attention_masks, pooling_name=\"cls\", hidden_states=1):\n",
    "    last_hidden_state = outputs.hidden_states[-1]\n",
    "    if hidden_states > 1:\n",
    "      last_hidden_state = torch.cat(tuple([outputs.hidden_states[-i] for i in range(hidden_states, 0, -1)]), dim=-1)\n",
    "\n",
    "    input_mask_expanded = attention_masks.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "    if pooling_name == \"mean\":\n",
    "        mean_pooling = torch.sum(last_hidden_state * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return mean_pooling\n",
    "    elif pooling_name == \"cls\":\n",
    "        cls_pooling = last_hidden_state[:, 0, :]\n",
    "        cls_pooling = torch.nn.functional.normalize(cls_pooling)\n",
    "        return cls_pooling\n",
    "    elif pooling_name == \"max\":\n",
    "       max_pooling, _ = torch.max(last_hidden_state * input_mask_expanded, dim=1)\n",
    "       return max_pooling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for multi-task learning\n",
    "class ModelMTL(nn.Module):\n",
    "    def __init__(self, model_name, num_error_types, num_hidden_states=1, pooling_name=\"cls\", freeze=False):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "        self.pooling_name = pooling_name\n",
    "        self.num_hidden_states = num_hidden_states\n",
    "        self.freeze = freeze\n",
    "        self.binary_classifier = nn.Linear(self.num_hidden_states * self.model.config.hidden_size, 2)\n",
    "        self.error_classifier = nn.Linear(self.num_hidden_states * self.model.config.hidden_size, num_error_types)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Freeze encoder\n",
    "        if self.freeze:\n",
    "            for layer in self.model.encoder.layer[:-3]: \n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        frozen_params = total_params - trainable_params\n",
    "\n",
    "        print(f\"Trainable parameters: {trainable_params / 1e6:.2f}M\")\n",
    "        print(f\"Frozen parameters: {frozen_params / 1e6:.2f}M\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        pooled_output = get_pooling(outputs, attention_mask, pooling_name=self.pooling_name, hidden_states=self.num_hidden_states)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # out for binary clf\n",
    "        binary_logits = self.binary_classifier(pooled_output)\n",
    "        # out for multi-class clf\n",
    "        error_logits = self.error_classifier(pooled_output)\n",
    "        \n",
    "        return binary_logits, error_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.4\n",
    "\n",
    "binary_target = \"is_mistake\"\n",
    "multi_target = \"type_mistake\"\n",
    "\n",
    "binary_class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                    classes=np.unique(train_df[binary_target].values),\n",
    "                                                    y=train_df[binary_target].values)\n",
    "multi_class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                    classes=np.unique(train_df[train_df[multi_target]!=-1][multi_target].values),\n",
    "                                                    y=train_df[train_df[multi_target]!=-1][multi_target].values)\n",
    "\n",
    "binary_label_weights = {id: weight for weight, id in zip(np.unique(train_df[binary_target].values), binary_class_weights)}\n",
    "multi_label_weights = {id: weight for weight, id in zip(np.unique(train_df[train_df[multi_target]!=-1][multi_target].values), multi_class_weights)}\n",
    "\n",
    "def mtl_loss(binary_logits, error_logits, binary_labels, error_labels, weight_error=ALPHA, loss_type='default'):\n",
    "\n",
    "    if loss_type == 'balanced':\n",
    "        binary_weighted_loss = nn.CrossEntropyLoss(weight=torch.from_numpy(binary_class_weights).float().to(device))\n",
    "        multi_weighted_loss = nn.CrossEntropyLoss(weight=torch.from_numpy(multi_class_weights).float().to(device))\n",
    "    else:\n",
    "        binary_weighted_loss = nn.CrossEntropyLoss()\n",
    "        multi_weighted_loss =  nn.CrossEntropyLoss()\n",
    "\n",
    "    loss_binary = binary_weighted_loss(binary_logits, binary_labels.long())\n",
    "    \n",
    "    mask = (binary_labels == 1)\n",
    "    if mask.sum() > 0: \n",
    "        masked_error_logits = error_logits[mask]\n",
    "        masked_error_labels = error_labels[mask].long()\n",
    "        \n",
    "        loss_error = multi_weighted_loss(\n",
    "            masked_error_logits,\n",
    "            masked_error_labels\n",
    "        )\n",
    "    else:\n",
    "        loss_error = torch.tensor(0.0, device=binary_logits.device)\n",
    "    \n",
    "    # total_loss = loss_binary + weight_error * loss_error\n",
    "    total_loss = (1 - weight_error) * loss_binary + weight_error * loss_error\n",
    "    \n",
    "    return total_loss, loss_binary, loss_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def train(model, dataloader, optimizer, device, writer, epoch, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        binary_labels = batch['binary_label'].to(device)\n",
    "        error_labels = batch['error_label'].to(device)\n",
    "        \n",
    "        binary_logits, error_logits = model(input_ids, attention_mask)\n",
    "        loss, loss_bin, loss_err = mtl_loss(\n",
    "            binary_logits, error_logits, \n",
    "            binary_labels, error_labels\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Log in tensorboard\n",
    "        if writer:\n",
    "            writer.add_scalar('Loss/train', loss.item(), epoch * len(dataloader) + step)\n",
    "            writer.add_scalar('Loss/train_binary', loss_bin.item(), epoch * len(dataloader) + step)\n",
    "            writer.add_scalar('Loss/train_error', loss_err.item(), epoch * len(dataloader) + step)\n",
    "            if scheduler:\n",
    "                writer.add_scalar('Learning Rate', scheduler.get_last_lr()[0], epoch * len(dataloader) + step)\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Validation\n",
    "def evaluate(model, dataloader, device, writer, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    binary_preds, binary_labels = [], []\n",
    "    error_preds, error_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            binary_labels_batch = batch['binary_label'].to(device)\n",
    "            error_labels_batch = batch['error_label'].to(device)\n",
    "            \n",
    "            binary_logits, error_logits = model(input_ids, attention_mask)\n",
    "            loss, loss_bin, loss_err = mtl_loss(\n",
    "                binary_logits, error_logits, \n",
    "                binary_labels_batch, error_labels_batch\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Save prds\n",
    "            binary_preds.extend(torch.argmax(binary_logits, dim=1).cpu().numpy())\n",
    "            binary_labels.extend(binary_labels_batch.cpu().numpy())\n",
    "            error_preds.extend(torch.argmax(error_logits, dim=1).cpu().numpy())\n",
    "            error_labels.extend(error_labels_batch.cpu().numpy())\n",
    "    \n",
    "    # Metrics for binary clf\n",
    "    binary_accuracy = accuracy_score(binary_labels, binary_preds)\n",
    "    mcc = matthews_corrcoef(binary_labels, binary_preds)\n",
    "    binary_f1 = f1_score(binary_labels, binary_preds, average='binary')\n",
    "    binary_precision = precision_score(binary_labels, binary_preds,  average='binary')\n",
    "    binary_recall = recall_score(binary_labels, binary_preds, average='binary')\n",
    "    \n",
    "    # Metrics for multi-class clf\n",
    "    error_mask = [label != -1 for label in error_labels]  # Exclude correct texts\n",
    "    error_preds_filtered = [p for p, m in zip(error_preds, error_mask) if m]\n",
    "    error_labels_filtered = [l for l, m in zip(error_labels, error_mask) if m]\n",
    "    \n",
    "    if error_labels_filtered:\n",
    "        error_accuracy = accuracy_score(error_labels_filtered, error_preds_filtered)\n",
    "        error_f1 = f1_score(error_labels_filtered, error_preds_filtered, average='macro')\n",
    "        error_mcc = matthews_corrcoef(error_labels_filtered, error_preds_filtered)\n",
    "        error_report = classification_report(error_labels_filtered, error_preds_filtered)\n",
    "    else:\n",
    "        error_accuracy, error_f1, error_report = 0, 0, \"No errors in validation set\"\n",
    "    \n",
    "    # Log in tensorboard\n",
    "    if writer:\n",
    "        writer.add_scalar('Loss/val', total_loss / len(dataloader), epoch)\n",
    "        writer.add_scalar('Accuracy/binary_val', binary_accuracy, epoch)\n",
    "        writer.add_scalar('MCC/binary_val', mcc, epoch)\n",
    "        writer.add_scalar('F1/binary_val', binary_f1, epoch)\n",
    "        writer.add_scalar('Accuracy/error_val', error_accuracy, epoch)\n",
    "        writer.add_scalar('MCC/error_val', error_mcc, epoch)\n",
    "        writer.add_scalar('F1/error_val', error_f1, epoch)\n",
    "    \n",
    "    # Metrics output\n",
    "    print(f\"Validation Loss: {total_loss / len(dataloader):.3f}\")\n",
    "    print(f\"Binary Accuracy: {binary_accuracy:.3f}, F1: {binary_f1:.3f}, MCC: {mcc:.3f}\")\n",
    "    print(f\"Error Accuracy: {error_accuracy:.3f}, F1: {error_f1:.3f}, MCC: {error_mcc:.3f}\")\n",
    "    # print(\"Error Classification Report:\")\n",
    "    # print(error_report)\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"DeepPavlov/rubert-base-cased\"\n",
    "model_name = \"RussianNLP/ruRoBERTa-large-rucola\"\n",
    "# model_name = \"ai-forever/ru-en-RoSBERTa\"\n",
    "max_length = 128\n",
    "batch_size = 16\n",
    "num_error_types = 6\n",
    "lr = 2e-5\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at RussianNLP/ruRoBERTa-large-rucola and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 90.84M\n",
      "Frozen parameters: 264.52M\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_texts = train_df.text.values.tolist()\n",
    "train_labels = train_df.is_mistake.values.tolist()\n",
    "train_errors = train_df.type_mistake.values.tolist()\n",
    "train_dataset = TextDataset(train_texts, train_labels, train_errors, tokenizer, max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "val_texts = test_df.text.values.tolist()\n",
    "val_labels = test_df.is_mistake.values.tolist()\n",
    "val_errors = test_df.type_mistake.values.tolist()\n",
    "val_dataset = TextDataset(val_texts, val_labels, val_errors, tokenizer, max_length)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = ModelMTL(model_name, num_error_types, pooling_name='cls', freeze=True).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('./runs', datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "save_model_path = \"./trained_models/\" + model_name.split('/')[1] + f\"_lr-{lr}\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "print(f\"Model name: {model_name.split('/')[1]}\")\n",
    "\n",
    "# Train-val cycle\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_dataloader, optimizer, device, writer, epoch, scheduler=scheduler,)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} Train Loss: {train_loss:.3f}\")\n",
    "    \n",
    "    # Val\n",
    "    val_loss = evaluate(model, val_dataloader, device, writer, epoch)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} Validation Loss: {val_loss:.3f}\")\n",
    "\n",
    "    # Save model\n",
    "    # torch.save(model.state_dict(), f\"{save_model_path}_epoch-{epoch}.pt\")\n",
    "    # print(f\"Model saved to {save_model_path}_epoch-{epoch}\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rubert-base-cased best\n",
    "# Binary Accuracy: 0.787, F1: 0.731, MCC: 0.600\n",
    "# Error Accuracy: 0.372, F1: 0.206, MCC: 0.162\n",
    "# Epoch 13/20 Validation Loss: 1.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ru-en-RoSBERTa\n",
    "# Binary Accuracy: 0.701, F1: 0.642, MCC: 0.405\n",
    "# Error Accuracy: 0.298, F1: 0.169, MCC: 0.072\n",
    "# Epoch 11/20 Validation Loss: 1.047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ruRoBERTa-large-rucola\n",
    "# Binary Accuracy: 0.736, F1: 0.644, MCC: 0.512\n",
    "# Error Accuracy: 0.394, F1: 0.239, MCC: 0.215\n",
    "# Epoch 9/20 Validation Loss: 1.024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sbert_large_nlu_ru\n",
    "# Binary Accuracy: 0.772, F1: 0.720, MCC: 0.558\n",
    "# Error Accuracy: 0.383, F1: 0.171, MCC: 0.176\n",
    "# Epoch 14/20 Validation Loss: 1.011"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
