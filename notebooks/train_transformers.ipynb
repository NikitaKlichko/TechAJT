{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from torch.utils.data import Dataset\n",
    "from transformers.trainer_utils import SchedulerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_ajt_df.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"test_ajt_df.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ajt_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, df.loc[train_df.index]['type_mistake'], ], axis=1)\n",
    "test_df = pd.concat([test_df, df.loc[test_df.index]['type_mistake'], ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapper = {\"нет ошибки\": 0, \"речевая\": 1, \"стилистическая\": 2, \"пунктуационная\": 3, \"грамматическая\": 4, \"лексическая\": 5, \"логическая\": 6}\n",
    "reverse_label_mapper = {0: \"нет ошибки\", 1: \"речевая\", 2: \"стилистическая\", 3: \"пунктуационная\", 4: \"грамматическая\", 5: \"лексическая\", 6: \"логическая\"}\n",
    "\n",
    "# label_mapper = {\"нет ошибки\": -1, \"речевая\": 0, \"стилистическая\": 1, \"пунктуационная\": 2, \"грамматическая\": 3, \"лексическая\": 4, \"логическая\": 5}\n",
    "# reverse_label_mapper = {-1: \"нет ошибки\", 0: \"речевая\", 1: \"стилистическая\", 2: \"пунктуационная\", 3: \"грамматическая\", 4: \"лексическая\", 5: \"логическая\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['type_mistake'] = train_df['type_mistake'].map(label_mapper)\n",
    "test_df['type_mistake'] = test_df['type_mistake'].map(label_mapper)\n",
    "\n",
    "# # if multi\n",
    "# train_df = train_df[train_df['type_mistake'] != -1]\n",
    "# test_df = test_df[test_df['type_mistake'] != -1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if ru-en-RosBerta\n",
    "# prefix = \"classification: \"\n",
    "# train_df['text'] = prefix + train_df['text'] \n",
    "# test_df['text'] = prefix + test_df['text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if binary\n",
    "target_col = \"is_mistake\"\n",
    "drop_col = \"type_mistake\"\n",
    "\n",
    "# # if multi\n",
    "# target_col = \"type_mistake\"\n",
    "# drop_col = \"is_mistake\"\n",
    "\n",
    "train_df = train_df.rename(columns={target_col: \"label\"}).drop(columns=[drop_col])\n",
    "test_df = test_df.rename(columns={target_col: \"label\"}).drop(columns=[drop_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"DeepPavlov/rubert-base-cased\"\n",
    "# MODEL_NAME = \"RussianNLP/ruRoBERTa-large-rucola\"\n",
    "# MODEL_NAME = \"ai-forever/ru-en-RoSBERTa\"\n",
    "MAX_LENGHT = 128\n",
    "BS = 16\n",
    "if binary\n",
    "NUM_LABELS = 2\n",
    "# # if multi\n",
    "# NUM_LABELS = 6\n",
    "LR = 2e-5\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx][\"text\"]\n",
    "        label = self.data.iloc[idx][\"label\"]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "    if NUM_LABELS == 2: \n",
    "        f1 = f1_score(labels, predictions, average=\"binary\")\n",
    "    else:\n",
    "        f1 = f1_score(labels, predictions, average=\"macro\")\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1\": f1,\n",
    "        \"MCC\": mcc,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"./trans_clf\"\n",
    "LOG_DIR = \"./trans_clf_logs\"\n",
    "WM_RATIO = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = DataFrameDataset(train_df, tokenizer, MAX_LENGHT)\n",
    "val_dataset = DataFrameDataset(test_df, tokenizer, MAX_LENGHT)\n",
    "\n",
    "# # binary\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if roberta-multi\n",
    "# class MultiClassClassifier(torch.nn.Module):\n",
    "#     def __init__(self, model_name, num_labels=6):\n",
    "#         super().__init__()\n",
    "#         self.model = AutoModel.from_pretrained(model_name)\n",
    "#         self.dropout = torch.nn.Dropout(0.1)\n",
    "#         self.error_classifier = torch.nn.Linear(self.model.config.hidden_size, num_labels)\n",
    "    \n",
    "#     def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "#         outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         pooled_output = outputs.pooler_output # cls\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "#         logits = self.error_classifier(pooled_output)\n",
    "\n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             loss_fct = torch.nn.CrossEntropyLoss()\n",
    "#             loss = loss_fct(logits, labels)\n",
    "\n",
    "#         return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "# model = MultiClassClassifier(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREEZE = True\n",
    "\n",
    "if FREEZE:\n",
    "    for layer in model.model.encoder.layer[:-1]: \n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params / 1e6:.2f}M\")\n",
    "print(f\"Frozen parameters: {frozen_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BS,\n",
    "    per_device_eval_batch_size=BS,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    save_strategy=\"no\",\n",
    "    logging_dir=LOG_DIR,\n",
    "    logging_strategy=\"epoch\",\n",
    "    lr_scheduler_type=SchedulerType.LINEAR,\n",
    "    warmup_ratio=WM_RATIO,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rubert-base-cased\"\n",
    "# binary\n",
    "# Validation Loss\tAccuracy\tF1\tMcc\n",
    "# 0.567981\t0.725888\t0.706522\t0.449981\n",
    "# multi\n",
    "# 1.645356\t0.351064\t0.189984\t0.126034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ruRoBERTa-large-rucola\n",
    "# binary\n",
    "# Validation Loss\tAccuracy\tF1\tMcc\n",
    "# 0.655968\t0.695431\t0.552239\t0.452574\n",
    "# multi\n",
    "# 1.687713\t0.361702\t0.210077\t0.156871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ru-en-RoSBERTa\n",
    "# binary\n",
    "# Validation Loss\tAccuracy\tF1\tMcc\n",
    "# 0.575851\t0.725888\t0.689655\t0.451698\n",
    "# multi\n",
    "# 1.682512\t0.287234\t0.149063\t0.030868"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
