{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from sklearn.utils import class_weight\n",
    "from torch.utils.data import Dataset\n",
    "from transformers.trainer_utils import SchedulerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_ajt_df.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"test_ajt_df.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ajt_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, df.loc[train_df.index]['type_mistake'], ], axis=1)\n",
    "test_df = pd.concat([test_df, df.loc[test_df.index]['type_mistake'], ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if binary\n",
    "label_mapper = {\"нет ошибки\": 0, \"речевая\": 1, \"стилистическая\": 2, \"пунктуационная\": 3, \"грамматическая\": 4, \"лексическая\": 5, \"логическая\": 6}\n",
    "reverse_label_mapper = {0: \"нет ошибки\", 1: \"речевая\", 2: \"стилистическая\", 3: \"пунктуационная\", 4: \"грамматическая\", 5: \"лексическая\", 6: \"логическая\"}\n",
    "# if multi\n",
    "# label_mapper = {\"нет ошибки\": -1, \"речевая\": 0, \"стилистическая\": 1, \"пунктуационная\": 2, \"грамматическая\": 3, \"лексическая\": 4, \"логическая\": 5}\n",
    "# reverse_label_mapper = {-1: \"нет ошибки\", 0: \"речевая\", 1: \"стилистическая\", 2: \"пунктуационная\", 3: \"грамматическая\", 4: \"лексическая\", 5: \"логическая\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['type_mistake'] = train_df['type_mistake'].map(label_mapper)\n",
    "test_df['type_mistake'] = test_df['type_mistake'].map(label_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if binary\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                    classes=np.unique(train_df['is_mistake'].values),\n",
    "                                                    y=train_df['is_mistake'].values)\n",
    "\n",
    "# if multi\n",
    "# train_df = train_df[train_df['type_mistake'] != -1]\n",
    "# test_df = test_df[test_df['type_mistake'] != -1 ]\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "#                                                     classes=np.unique(train_df['type_mistake'].values),\n",
    "#                                                     y=train_df['type_mistake'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if ru-en-RosBerta\n",
    "# prefix = \"classification: \"\n",
    "# train_df['text'] = prefix + train_df['text'] \n",
    "# test_df['text'] = prefix + test_df['text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if binary\n",
    "target_col = \"is_mistake\"\n",
    "drop_col = \"type_mistake\"\n",
    "\n",
    "# # if multi\n",
    "# target_col = \"type_mistake\"\n",
    "# drop_col = \"is_mistake\"\n",
    "\n",
    "train_df = train_df.rename(columns={target_col: \"label\"}).drop(columns=[drop_col])\n",
    "test_df = test_df.rename(columns={target_col: \"label\"}).drop(columns=[drop_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"DeepPavlov/rubert-base-cased\"\n",
    "# MODEL_NAME = \"RussianNLP/ruRoBERTa-large-rucola\"\n",
    "# MODEL_NAME = \"ai-forever/ru-en-RoSBERTa\"\n",
    "MAX_LENGHT = 128\n",
    "BS = 16\n",
    "# # if binary\n",
    "NUM_LABELS = 2\n",
    "# # if multi\n",
    "# NUM_LABELS = 6\n",
    "LR = 2e-5\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx][\"text\"]\n",
    "        label = self.data.iloc[idx][\"label\"]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "    if NUM_LABELS == 2: \n",
    "        f1 = f1_score(labels, predictions, average=\"binary\")\n",
    "    else:\n",
    "        f1 = f1_score(labels, predictions, average=\"macro\")\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1\": f1,\n",
    "        \"MCC\": mcc,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"./trans_clf\"\n",
    "LOG_DIR = \"./trans_clf_logs\"\n",
    "WM_RATIO = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = DataFrameDataset(train_df, tokenizer, MAX_LENGHT)\n",
    "val_dataset = DataFrameDataset(test_df, tokenizer, MAX_LENGHT)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if roberta-multi\n",
    "# class MultiClassClassifier(torch.nn.Module):\n",
    "#     def __init__(self, model_name, num_labels=6):\n",
    "#         super().__init__()\n",
    "#         self.model = AutoModel.from_pretrained(model_name)\n",
    "#         self.dropout = torch.nn.Dropout(0.1)\n",
    "#         self.error_classifier = torch.nn.Linear(self.model.config.hidden_size, num_labels)\n",
    "    \n",
    "#     def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "#         outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         pooled_output = outputs.pooler_output # cls\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "#         logits = self.error_classifier(pooled_output)\n",
    "\n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             loss_fct = torch.nn.CrossEntropyLoss()\n",
    "#             loss = loss_fct(logits, labels)\n",
    "\n",
    "#         return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "# model = MultiClassClassifier(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 99.89M\n",
      "Frozen parameters: 77.97M\n"
     ]
    }
   ],
   "source": [
    "FREEZE = True\n",
    "\n",
    "if FREEZE:\n",
    "    for layer in model.bert.encoder.layer[:-1]: \n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params / 1e6:.2f}M\")\n",
    "print(f\"Frozen parameters: {frozen_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=torch.from_numpy(class_weights).float().to(device))\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/nk/dl2024/train_env/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BS,\n",
    "    per_device_eval_batch_size=BS,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    save_strategy=\"no\",\n",
    "    logging_dir=LOG_DIR,\n",
    "    logging_strategy=\"epoch\",\n",
    "    lr_scheduler_type=SchedulerType.LINEAR,\n",
    "    warmup_ratio=WM_RATIO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BALANCED= False\n",
    "if BALANCED:\n",
    "    trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights\n",
    ")\n",
    "else:\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rubert-base-cased\"\n",
    "# binary\n",
    "# Validation Loss\tAccuracy\tF1\tMcc\n",
    "# 0.567981\t0.725888\t0.706522\t0.449981\n",
    "# multi\n",
    "# 1.645356\t0.351064\t0.189984\t0.126034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ruRoBERTa-large-rucola\n",
    "# binary\n",
    "# Validation Loss\tAccuracy\tF1\tMcc\n",
    "# 0.655968\t0.695431\t0.552239\t0.452574\n",
    "# multi\n",
    "# 1.687713\t0.361702\t0.210077\t0.156871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ru-en-RoSBERTa\n",
    "# binary\n",
    "# Validation Loss\tAccuracy\tF1\tMcc\n",
    "# 0.575851\t0.725888\t0.689655\t0.451698\n",
    "# multi\n",
    "# 1.682512\t0.287234\t0.149063\t0.030868"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
